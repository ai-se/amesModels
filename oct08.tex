\documentclass[12pt]{article}

\usepackage{fullpage} 
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{url}

\addtolength{\itemsep}{-0.05in}
\usepackage[small,compact]{titlesec}
\usepackage[small,it]{caption}
 


\newcommand{\bi}{\begin{itemize}[leftmargin=0.7cm]}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}[leftmargin=0.7cm]}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
 

\usepackage[shortlabels]{enumitem}  

\usepackage{cite} 
\def\baselinestretch{0.92}


\setlist{nosep}
  
 \usepackage[font={small}]{caption, subfig}
\setlength{\abovecaptionskip}{1ex}
 \setlength{\belowcaptionskip}{1ex}
 \setlength{\floatsep}{1ex}
 \setlength{\textfloatsep}{1ex}
\usepackage[compact,small]{titlesec}
 



\begin{document} 
\begin{center}
{\Large {\bf  Understanding Huge  Models (of NAS)}}\\
Tim Menzies (PI), Computer Science, NC State, USA tim.menzies@gmail.com\\ 
Neha Rungta,  NASA AMES,USA  neharungta@gmail.com
\end{center}
\noindent
\underline{{\bf THE DOMAIN:}}
NASA is using model-based methods to reason about complex systems. At NASA AMES,
such models include detailed simulations:
\bi
\item Astronauts flying the ORION aircraft to Mars. 
\item Pilots landing commercial aircraft throw crowded
airpsaces at  LaGuardia Airport (this work is part of the Next Generation Air Transportation System);
\ei
Utilizing NASA's supercomputer facilities, it is possible to simulate these models to find ways to find and fix problems associated with : 
\bi
\item Pilot operation procedures that lead to safety violations;
\item Dangerous spikes in operator workload;
\item Undesired large deviations in performance (e.g. flight delays).
\ei 
But can we trust the conclusions achieved via these models?

\noindent
\underline{{\bf THE PROBLEM:}} Before we can trust the output of a model, we must trust the model itself. 
ACT-R, SOAR, and other architectures provide the ability to model actual low-level cognitive processes~\cite{anderson1997act,laird1987soar,lebiere2013cognitive,liu2009qn,lundinsimulating}. However, 
such modeling languages cannot  describe complex systems and interactions at a higher-level of abstraction, or   the decision making process of the human~\cite{pritchett2011simulating}. 
Other modeling tools, like
the BRAHMS tool used at NASA AMES~\cite{Rungta:2013,Clancey:1998:BSP:306180.306196,brahms-semantics,brahms-jelia,SierhuisPhD} can build more extensive models that know about higher-level human cognitive reasoning. Hence, they are the 
  tool of choice at   AMES for modeling pilots flying to  New York or  Mars.

The problem with  these large BRAHMS  models is that they contain   more extensive interactions. Hence, when verifying them,  it is   required to severely constrain the analysis.
For example, recent work restricted the human interactions and activity in a single sector, and the air traffic controller actions to a few  rigidly pre-defined actions~\cite{MIDAS}. 

If we want to understand   the interactions inside complex NASA models that represent   (e.g.) pilots flying to New York or Mars, then we could
use automatic verification tools. Decades of research at NASA has lead to rigorous and formal verification methods for model-based reasoning. Unfortunately, all those methods suffer from problems of scale.
\bi
\item
{\em Theorem provers} require complex models to be augmented with extensive annotations describing their properties. The search space within theorems can be so large, that even state-of-the-art  provers are defeated by huge   models~\cite{Rungta:2013}.
\item
{\em Model checkers}  scale to larger models than theorem proves but   can only test if a small number of most critical assertions can be violated in the most critical regions of large BRAHMS models. Like theorem provers, model checkers
have issues scaling up to huge models.
\item
{\em Runtime verification} is a   promising technology (since it can
be automatically applied to huge programs)  but, without a guide
on how to direct its search, runtime verification   fall back to an exhaustive
search through all combinations-- which is impractical for
huge models.
\ei
\noindent
\underline{{\bf OUR PROPOSED SOLUTION:}} We propose to find the guides
that can assist runtime verification in exploring these huge models. Our approach is based on the following premise: XXX


scale to larger models than model checkers. That
said, given the size of the models now being studied at NASA, these tools still suffer
from runtime limitations.
\ei
The problem with all the above is   state space explosion: in theory,
$N$ variables each with $V$ different values has a total space of $V^N$. In practice,
this number can be so large that it cannot be rigorously explored. For example, a system 
with 300 boolean variables (N=300,V=2) has $2^300$ states; i.e. more states that stars in the observable universe.


A recent report~\cite{nrc:nextgen} on the Next Generation Air Transportation System (NexGen) within the NAS discusses the lack of models that accurately capture the decision making process and actions of the human operators. The report states, \emph{``One might like to use fast-time analytic models and simulations, but unfortunately there are few human-system analytic models that are very predictive, and they are also very context sensitive. There are very
few analytic models that are up to being very helpful for NextGen, other than for modeling basic vision and hearing.''} The report states that the lack of such human-analytic models will be a problem area for the Federal Aviation Administration (FAA) and NASA as they move toward to the modernization of the NAS in the NextGen. One of the biggest challenge in the development of fast-time simulation that captures human behavior and decision making process is the trade-off between (a) fidelity of the modeled system with respect to the real-world system versus (b) the considerations of computation efficiency and tractability of performing analysis on such models. In this project we propose developing analysis techniques that can scale to complex human operator models of the NAS and are predictive with respect to the safety and performance of the modeled system. 


\section{Related Workd}

In the area of civil aviation, measuring a human operator's mental
state presents a range of challenges to researchers despite various
proposals including subjective rating scales \cite{NASATLX88},
secondary task performance \cite{DualTaskMethodology91}, and
psychophysiological measures such as eye movements, heart rate, and
respiration \cite{MetricsWorkloadKramer91}).  Further, traditional
hu\-man\--in\--the\--lo\-op (HI\-TL) evaluations such as the NASA Task
Load Index (TLX) cannot adequately test performance across the
operational envelopes of complex work environments during the design
stage of the system development cycle.



\section{Predictive Human operator models} 

In this work we propose the use of MAS models as the basis of
characterizing the decision making process, actions, workload, skill,
interactions with automation, and situational awareness of human
operators.  MAS models offer an ideal design abstraction for systems
involving both humans and automation. These models provide the ability
for predictive reasoning about various safety conditions such as
expected behavior of the autonomy, situational awareness of humans,
workload of the human operators, and the amount of time taken from the
start to the end of a complete or partial operation or procedure.

Brahms is a simulation and development environment originally designed
to model the contextual situated activity behavior of groups of people
in a real world context~\cite{clancey1998brahms,SierhuisPhD}. In previous work, we created Brahms models for conditions that led to the Air France 447 accident~\cite{hunter:aamas13}, sequence of events that led to the \"{U}berlingen collision~\cite{Rungta:2013}, and performed a comparative evaluation of operator workload between a single pilot operations concept versus a traditional two pilot operation~\cite{Stocker:2015}. More recently, we have developed Brahms models for arrivals and departures at the La Guardia airport based on the work on Departure Sensitive Arrival Scheduling (DSAS) concept developed by the Airspace Operations Lab (AOL) at NASA Ames~\cite{dsas}. 

The DSAS concept provides the ability to maximize departure throughput at LGA without impacting the flow of the arrival traffic; it was part of a research effort to explore NextGen TBO solutions to problems in the New York metroplex. The concept was prototyped in an HITL performed in 2014 that consider operational procedures related to co-ordination, timing, TSS schedule, and other display features available to controllers and supervisors at Center, TRACON, and Tower. The HITL results demonstrate that the DSAS operations have the potential to increase departure throughput at LGA by nine aircraft per hour with insignificant impact on arrivals~\cite{dsas}.  It was our goal to replicate the results of the study within the agent based framework of Brahms. Each scenario evaluated in the HILT of the DSAS study consists of approximately 1.5 hours real time of simulation. During this time, there are between 130 and 150 airplanes being managed by four enroute controllers, three TRACON controllers, and one tower controller at LGA who is responsible for departures and arrivals. The planes are landing at approximately 36 to 40 planes an hour. 

\subsection{Analysis of MAS models} 
Due to the various decision choices in the model even a single simulation run can take several minutes. This makes any form of exhaustive, proof derivation based techniques based on model checking, theorem-proving or other compraable approaches infeasible to apply~\cite{hunter:aamas13,raimondi:aamas14}. The goal of this work is develop techniques that allow us to successfully analyze the models human system interaction models such that they can be used to predict stuff about the modeled system. 

NASA is struggling to reason about very big, very complex models. NEHA! 

If we can reason about these models then NEHA! But if the complexity of those models
defeat us then NEHA!

Over the last twenty years, various
NASA research teams have tried solving this problem using a variety of techniques:
\bi
\item Theorem provers are a promising technology but require complex models to be augmented with extensive annotations describing their properties. Worse, the search space within theorems relating to real-world models is so large, that even state-of-the-art theorem provers cannot handle very large models.
\item Model checkers can scale to larger models. Such model checkers can test if a small number of most critical assertions can be violated. Like theorem provers, model checkers
have issues scaling up to large models.
\item Runtime verification tools can scale to larger models than model checkers. That
said, given the size of the models now being studied at NASA, these tools still suffer
from runtime limitations.
\ei
The problem with all the above is   state space explosion: in theory,
$N$ variables each with $V$ different values has a total space of $V^N$. In practice,
this number can be so large that it cannot be rigorously explored. For example, a system 
with 300 boolean variables (N=300,V=2) has $2^300$ states; i.e. more states that stars in the observable universe.

\section{Our Solution}

The solution, therefore, is to limit the state space. One approach to learning those limits
is the use of a {\em secondary oracle} that comments ``of all the states, these are more interesting than those''. For example, bounded model checkers say that the most interesting states are the ones some small $k$ steps away from the current frontier. Note that this approach is only as fast  as the secondary oracle.
%  me12d boley98  Faloutsos1995 platt05 kamvar03 kamvar03

Hence, this research proposes limiting the state space explosion with a very fast secondary oracle called WHERE~\cite{me12d} and GALE~\cite{krall2015gale}. 
Specifically, 
our proposal is to use WHERE+GALE as an optimizer to learn constraints that select for better model behaviours and then run formal verification tools in that reduced space.

\underline{{\bf WHERE and GALE:}} 
Consider 
how most optimizers handle  $n$ model inputs:
\bi
\item
 All $n$ vectors are ``evaluated''; i.e. used as model input to
 generate   output performance scores. 
\item
Next, some   some $O(n^2)$ comparison process  selects the $m \subseteq n$ best vectors.
\ei
An faster approach, that uses fewer evaluations, is to cluster $n$ vectors
into a small number of clusters, then
 execute only a small sample of vectors in each cluster.
 Limiting the number of evaluations is vital to the success of large-scale modeling
 at NASA since recent experience has show that the kinds of models we wish to process
 are very slow indeed. NEHA!
 
 
For this work, we propose a top-down  {\em binary spectral clustering} algorithm~\cite{kamvar03} 
that recursively divides
the state space along their eigenvectors. Eigenvectors are useful since 
  $d$ variables can usually be compressed to just
  a few $e \ll d$ eigenvectors. This approach handles:
\bi
\item
Multiple {\em correlated variables} $i,j,k \subseteq d$, which can be represented
by a single  eigenvector.
\item
{\em Noisy variables} from $d$, which are 
ignored since they  do not contribute to the signal in the data.
\item
{\em Redundant  variables} \mbox{$i,j \in d$} which become (approximately) parallel lines
in $e$ space. Hence, we 
can ignore $j$ 
since effects that change over $j$ all
change   the same way over $i$.
\ei
Standard top-down spectral learners require  $O(n^2)$ calculations at each level 
of the recursion to find the eigenvectors~\cite{boley98}. A faster method, called WHERE 
uses Faloutsos'  heuristic~\cite{Faloutsos1995} to
approximate eigenvectors   in $O(n)$ time~\cite{platt05}.
WHERE  is a top-down binary clusterer.  
At each level, WHERE  projects all  vectors onto a line connecting two distant vectors called $P,Q$ (generating this line takes $O(n)$ time\footnote{Faloutsosâ€™ heuristic: Pick any vector $O$ at random. Let one pole $P$ by the
vector farthest from $O$. Let the other pole $Q$ be the vector farthest from $P$. Note that
finding $P,Q$ requires only $2n$ distance calculations across $n$ vectors.}).
The line is then divided in half
and WHERE recurses on each half. 
%To  find  small clusters of size, say, $m=\sqrt{n}$,
%WHERE   build a binary tree of height $log_2\sqrt{n}$ with 
%$2^{log_2\sqrt{n}}=\sqrt{n}$ nodes. Each node   evaluates only two vectors $P,Q$; i.e.
%WHERE requires at most $2\sqrt{n}$ evaluations.

Using WHERE, we have build a very fast     evolutionary optimizer
called GALE~\cite{krall2015gale}. 
GALE is very fast since it uses the above clustering to find and cull much of the 
search space.
When dividing the data and 
evaluating   $P,Q$, if $P$ produces better model performance scores than $Q$, then
GALE  culls   the worst half of the data (this approach cuts back $n$ examples to $\sqrt{n}$ using
just $2*log_2{\sqrt{n}}$ evaluations). 
For the final leaf cluster, GALE says that the {\em direction of improvement} us
towards the pole
that generates better performance scores. GALE can then build $n$ new vectors by mutate
all the vectors in that leaf along the direction of improvement. 
These mutants become
generation $i+1$ that can be clustered  and then mutated again to form
generation $i+2$, etc.

In experiments with large NASA models~\cite{me15z}, GALE has found optimizations comparable to more standard methods, but hundreds of times faster (i.e. four minutes, not 7.5 hours). Note that WHERE+GALE make no assumptions of continuous or discrete-space vectors-- they
will run on either just as long a there exists some distance predicate that can comment on
the the similarity of pairs of input states.

\noindent{{\bf GALE and Verification:}}


Our proposal, therefore, is to combine 

{\bf About the PIs:} This is the right team to perform this week. NEHA has . PI Menzies has.


\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}