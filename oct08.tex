\documentclass[12pt]{article}

\usepackage{fullpage} 
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{url}

\addtolength{\itemsep}{-0.05in}
\usepackage[small,compact]{titlesec}
\usepackage[small,it]{caption}
 


\newcommand{\bi}{\begin{itemize}[leftmargin=0.7cm]}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}[leftmargin=0.7cm]}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
 

\usepackage[shortlabels]{enumitem}  

\usepackage{cite} 
\def\baselinestretch{0.92}


\setlist{nosep}
  
 \usepackage[font={small}]{caption, subfig}
\setlength{\abovecaptionskip}{1ex}
 \setlength{\belowcaptionskip}{1ex}
 \setlength{\floatsep}{1ex}
 \setlength{\textfloatsep}{1ex}
\usepackage[compact,small]{titlesec}
 



\begin{document} 
\begin{center}
{\Large {\bf  Understanding Huge  Models (of NAS)}}\\
Tim Menzies (PI), Computer Science, NC State, USA tim.menzies@gmail.com\\ 
Neha Rungta,  NASA AMES,USA  neharungta@gmail.com
\end{center}
\noindent
\underline{{\bf ABSTRACT:}}  

%We seek safer systems. NASA  uses model-based methods to find ways to improve safety in complex systems. 
The current National Airspace (NAS) is quite safe and accidents are at a record low. Over the past few years, the US has embarked in a transformation of the Air Transportation System (ATS) to address the expected increase of air traffic in the US. The goal is to increase efficiency without compromising safety. There is a critical need for new safety evaluation techniques to verify and validate the interactions of humans and autonomy in the context of new complex software systems and changing roles of the human operator. 
%But can we trust the conclusions achieved via these models? Some of these models are so large that they defeat formal verification tools. 
The current state of the art to assess interactions of humans and autonomy is to model the air transportation system as a distributed, interactive system of systems with authority and autonomy assigned to both humans and automation at multiple levels. The scale and complexity of these models make the use of traditional verification and validation techniques intractable. In this work, we propose a novel approach for combining heuristic optimisers with formal verification in order to better understand huge models. In this approach,  optimizers quickly select the smaller, most interesting regions of the model. Our hypothesis (to be tested in this work) is in this smaller space, it is possible that formal verification can completely explore the region of possibilities in the model and  characterize the trade-space between safety, efficiency, and workload on the human operators.

\underline{{\bf THE DOMAIN:}}
NASA and the FAA agree that the US need to modernize the ATS and to implement NextGen (Next Generation Air Transportation System) to accommodate the traffic increase over the next 15 years; Europe is going through a similar effort with SESAR. NextGen needs to provide at least the same, if not a better, level of safety. The  NextGen IWP has a requirement (R-1440) that calls for new and improved V\&V techniques for complex systems. 

Recent work at NASA Ames models several scenarios in the Brahms multi-agent framework~\cite{clancey1998brahms,SierhuisPhD}. Some examples include, Brahms models for conditions that led to the Air France 447 accident~\cite{hunter:aamas13}, sequence of events that led to the \"{U}berlingen collision~\cite{Rungta:2013}, and performed a comparative evaluation of operator workload between a single pilot operations concept versus a traditional two pilot
operation~\cite{Stocker:2015}. More recently, Brahms models have been developed for arrivals and departures at the La Guardia airport based on the work on Departure Sensitive Arrival Scheduling (DSAS) concept developed by the Airspace Operations Lab (AOL) at NASA Ames~\cite{dsas}. Through simulations of these models the interactions between pilots, controllers, and automation several issues regarding safety (Air France 447 and  \"{U}berlingen crashes); performance (DSAS concept), and human operator workload (single pilot operation) have been discovered, characterized, and fixed. 


%At NASA AMES, system safey is explored via simulations. The models being simulated
%here are  many and varied and include: 
%\bi 
%\item Astronauts flying the ORION aircraft to Mars. 
%\item Pilots landing commercial aircraft throw crowded
%airpsaces at  LaGuardia Airport (this work is part of the Next Generation Air Transportation System);
%\ei
%Utilizing NASA's supercomputer facilities, it is possible to simulate these models to find ways to find and fix problems associated with : 
%\bi
%\item Pilot operation procedures that lead to safety violations;
%\item Dangerous spikes in operator workload;
%\item Undesired large deviations in performance (e.g. flight delays).
%\ei 


\noindent
\underline{{\bf THE PROBLEM:}} 
%% nsr: validation of models is not something that we would want to add to the scope fo the current work -- it will get too complicated. 
%Before we can trust the output of a model, we must trust the model itself. 
ACT-R, SOAR, and other architectures provide the ability to model actual low-level cognitive processes~\cite{anderson1997act,laird1987soar,lebiere2013cognitive,liu2009qn,lundinsimulating}. However,  such modeling languages cannot  describe complex systems and interactions at a higher-level of abstraction, or   the decision making process of the human~\cite{pritchett2011simulating}. 
Other modeling tools, like the Brahms tool used at NASA Ames~\cite{Rungta:2013,Clancey:1998:BSP:306180.306196,brahms-semantics,brahms-jelia,SierhuisPhD} can build more extensive models that know about higher-level human cognitive reasoning. Hence, they are the tool of choice at Ames for modeling air traffic management (ATM), NAS operations, and even manned space flight scearios.

The problem with models is that even at a higher-level of abstraction, they can contain fairly extensive interactions as well as a large number of rational agents including their beliefs, goals, and actions. Hence, when verifying them, it is often required to severely constrain the analysis by either creating simpler models or bounding the scope of the verification.
%For example, recent work restricted the human interactions and activity in a single sector, and the air traffic controller actions to a few  rigidly pre-defined actions~\cite{MIDAS}. 
There is a pressing need to scale automated verification tools to meet the demands of NextGen. Decades of research at NASA has lead to rigorous and formal verification methods for model-based reasoning. Unfortunately, all those methods suffer from problems of scale. In this work we we focus on model checking based verification techniques.

\noindent
\underline{{\bf OUR PROPOSED SOLUTION:}} We propose to find the guides
that can assist model-checking based verification in exploring these huge models. Our approach is based on the  {\em principle of rationality}~\cite{Newell88} which states that experienced operators of any system know that certain actions are most relevant to important outcomes. That is, within the very large decision space of huge models, there exists a much smaller space of relevant and rational actions. If we could somehow find that smaller space, then that would reduce the problems of scale for formal verification. But how to find those smaller spaces?

One method is to use {\em  heuristic optimizers} such as the PI Menzies' GALE system~\cite{krall2015gale}.  When exploring a large space, GALE first applies a fast clustering algorithm to reduce  $n$ examples to $\sqrt{n}$ clusters\footnote{What is in in each cluster? model input?}.  These are sub-divided into a tree of clusters and sub-clusters and sub-sub-sub-clusters, etc. Next, GALE walks down that tree, pruning away sub-trees that contain the undesired model outputs. Undesired outputs include: (1)~conditions that lead to safety violations; (2)~dangerous spikes in operator workload; (3)~undesired large deviations in performance; e.g. flight delays. 

Note that GALE finds small clusters of decisions that contain desired model 
behavior. Such small clusters are ideal for formal verification tools since
within those spaces, the number of options are so constrained that verification
methods can run to termination. 
%Note also that if GALE cannot find clusters of desired behavior, it can also function as an optimiser.
Each cluster found by GALE has better or worse ends
contain subsets with better or worst behavior. GALE's evolutionary optimizer
mutates members of that cluster towards the better end. This becomes a new population of examples which can be past back to GALE for yet more pruning and
 mutation. In experiments with large NASA models~\cite{me15z}, GALE has found optimizations comparable to more standard methods, but hundreds of times faster (i.e. four minutes, not 7.5 hours).

 XXX not enough here. need to cycle around
 
 \textbf{Neha: this also needs a tie in to where the model checking part fits in}
 
\noindent
\underline{{\bf TECHNICAL DETAILS}} 
GALE uses  a top-down  {\em binary spectral clustering} algorithm~\cite{kamvar03} 
that recursively divides
the state space along their eigenvectors. Eigenvectors are useful since 
  $d$ variables can usually be compressed to just
  a few $e \ll d$ eigenvectors. This approach handles:
\bi
\item
Multiple {\em correlated variables} $i,j,k \subseteq d$, which can be represented
by a single  eigenvector.
\item
{\em Noisy variables} from $d$, which are 
ignored since they  do not contribute to the signal in the data.
\item
{\em Redundant  variables} \mbox{$i,j \in d$} which become (approximately) parallel lines
in $e$ space. Hence, we 
can ignore $j$ 
since effects that change over $j$ all
change   the same way over $i$.
\ei
Standard top-down spectral learners require  $O(n^2)$ calculations at each level 
of the recursion to find the eigenvectors~\cite{boley98}. A faster method, called  
uses Faloutsos'  heuristic~\cite{Faloutsos1995} to
approximate eigenvectors   in $O(n)$ time~\cite{platt05}.
GALE uses these eigenvectors for a top-down binary clusterer.  
At each level, GALE  projects all  vectors onto a line connecting two distant vectors called $P,Q$ (generating this line takes $O(n)$ time\footnote{Faloutsosâ€™ heuristic: Pick any vector $O$ at random. Let one pole $P$ by the
vector farthest from $O$. Let the other pole $Q$ be the vector farthest from $P$. Note that
finding $P,Q$ requires only $2n$ distance calculations across $n$ vectors.}).
The line is then divided in half
and GALE recurses on each half. 
%To  find  small clusters of size, say, $m=\sqrt{n}$,
%WHERE   build a binary tree of height $log_2\sqrt{n}$ with 
%$2^{log_2\sqrt{n}}=\sqrt{n}$ nodes. Each node   evaluates only two vectors $P,Q$; i.e.
%WHERE requires at most $2\sqrt{n}$ evaluations.

GALE is very fast since it uses the above clustering to find and cull much of the 
search space.
When dividing the data and 
evaluating   $P,Q$, if $P$ produces better model performance scores than $Q$, then
GALE  culls   the worst half of the data (this approach cuts back $n$ examples to $\sqrt{n}$ using
just $2*log_2{\sqrt{n}}$ evaluations). 
For the final leaf cluster, GALE says that the {\em direction of improvement} us
towards the pole
that generates better performance scores. GALE can then build $n$ new vectors by mutate
all the vectors in that leaf along the direction of improvement. 
These mutants become
generation $i+1$ that can be clustered  and then mutated again to form
generation $i+2$, etc.

 Note that WHERE+GALE make no assumptions of continuous or discrete-space vectors-- they
will run on either just as long a there exists some distance predicate that can comment on
the the similarity of pairs of input states.



scale to larger models than model checkers. That
said, given the size of the models now being studied at NASA, these tools still suffer
from runtime limitations.
\ei
The problem with all the above is   state space explosion: in theory,
$N$ variables each with $V$ different values has a total space of $V^N$. In practice,
this number can be so large that it cannot be rigorously explored. For example, a system 
with 300 boolean variables (N=300,V=2) has $2^300$ states; i.e. more states that stars in the observable universe.


A recent report~\cite{nrc:nextgen} on the Next Generation Air Transportation System (NexGen) within the NAS discusses the lack of models that accurately capture the decision making process and actions of the human operators. The report states, \emph{``One might like to use fast-time analytic models and simulations, but unfortunately there are few human-system analytic models that are very predictive, and they are also very context sensitive. There are very
few analytic models that are up to being very helpful for NextGen, other than for modeling basic vision and hearing.''} The report states that the lack of such human-analytic models will be a problem area for the Federal Aviation Administration (FAA) and NASA as they move toward to the modernization of the NAS in the NextGen. One of the biggest challenge in the development of fast-time simulation that captures human behavior and decision making process is the trade-off between (a) fidelity of the modeled system with respect to the real-world system versus (b) the considerations of computation efficiency and tractability of performing analysis on such models. In this project we propose developing analysis techniques that can scale to complex human operator models of the NAS and are predictive with respect to the safety and performance of the modeled system. 


\section{Related Workd}

In the area of civil aviation, measuring a human operator's mental
state presents a range of challenges to researchers despite various
proposals including subjective rating scales \cite{NASATLX88},
secondary task performance \cite{DualTaskMethodology91}, and
psychophysiological measures such as eye movements, heart rate, and
respiration \cite{MetricsWorkloadKramer91}).  Further, traditional
hu\-man\--in\--the\--lo\-op (HI\-TL) evaluations such as the NASA Task
Load Index (TLX) cannot adequately test performance across the
operational envelopes of complex work environments during the design
stage of the system development cycle.



\section{Predictive Human operator models} 

In this work we propose the use of MAS models as the basis of
characterizing the decision making process, actions, workload, skill,
interactions with automation, and situational awareness of human
operators.  MAS models offer an ideal design abstraction for systems
involving both humans and automation. These models provide the ability
for predictive reasoning about various safety conditions such as
expected behavior of the autonomy, situational awareness of humans,
workload of the human operators, and the amount of time taken from the
start to the end of a complete or partial operation or procedure.

Brahms is a simulation and development environment originally designed
to model the contextual situated activity behavior of groups of people
in a real world context~\cite{clancey1998brahms,SierhuisPhD}. In previous work, we created Brahms models for conditions that led to the Air France 447 accident~\cite{hunter:aamas13}, sequence of events that led to the \"{U}berlingen collision~\cite{Rungta:2013}, and performed a comparative evaluation of operator workload between a single pilot operations concept versus a traditional two pilot operation~\cite{Stocker:2015}. More recently, we have developed Brahms models for arrivals and departures at the La Guardia airport based on the work on Departure Sensitive Arrival Scheduling (DSAS) concept developed by the Airspace Operations Lab (AOL) at NASA Ames~\cite{dsas}. 

The DSAS concept provides the ability to maximize departure throughput at LGA without impacting the flow of the arrival traffic; it was part of a research effort to explore NextGen TBO solutions to problems in the New York metroplex. The concept was prototyped in an HITL performed in 2014 that consider operational procedures related to co-ordination, timing, TSS schedule, and other display features available to controllers and supervisors at Center, TRACON, and Tower. The HITL results demonstrate that the DSAS operations have the potential to increase departure throughput at LGA by nine aircraft per hour with insignificant impact on arrivals~\cite{dsas}.  It was our goal to replicate the results of the study within the agent based framework of Brahms. Each scenario evaluated in the HILT of the DSAS study consists of approximately 1.5 hours real time of simulation. During this time, there are between 130 and 150 airplanes being managed by four enroute controllers, three TRACON controllers, and one tower controller at LGA who is responsible for departures and arrivals. The planes are landing at approximately 36 to 40 planes an hour. 

\subsection{Analysis of MAS models} 
Due to the various decision choices in the model even a single simulation run can take several minutes. This makes any form of exhaustive, proof derivation based techniques based on model checking, theorem-proving or other compraable approaches infeasible to apply~\cite{hunter:aamas13,raimondi:aamas14}. The goal of this work is develop techniques that allow us to successfully analyze the models human system interaction models such that they can be used to predict stuff about the modeled system. 

NASA is struggling to reason about very big, very complex models. NEHA! 

If we can reason about these models then NEHA! But if the complexity of those models
defeat us then NEHA!

Over the last twenty years, various
NASA research teams have tried solving this problem using a variety of techniques:
\bi
\item Theorem provers are a promising technology but require complex models to be augmented with extensive annotations describing their properties. Worse, the search space within theorems relating to real-world models is so large, that even state-of-the-art theorem provers cannot handle very large models.
\item Model checkers can scale to larger models. Such model checkers can test if a small number of most critical assertions can be violated. Like theorem provers, model checkers
have issues scaling up to large models.
\item Runtime verification tools can scale to larger models than model checkers. That
said, given the size of the models now being studied at NASA, these tools still suffer
from runtime limitations.
\ei
The problem with all the above is   state space explosion: in theory,
$N$ variables each with $V$ different values has a total space of $V^N$. In practice,
this number can be so large that it cannot be rigorously explored. For example, a system 
with 300 boolean variables (N=300,V=2) has $2^300$ states; i.e. more states that stars in the observable universe.

\section{Our Solution}



\noindent{{\bf GALE and Verification:}}


Our proposal, therefore, is to combine 

{\bf About the PIs:} This is the right team to perform this week. NEHA has . PI Menzies has.


\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}


\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}

\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}

\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}

\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}

\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}

\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}

\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}

\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}

\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
\newpage
\bibliographystyle{plain}
\bibliography{refs}
\end{document}